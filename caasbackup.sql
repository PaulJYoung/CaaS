-- MySQL dump 10.13  Distrib 5.6.31, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: CaaS
-- ------------------------------------------------------
-- Server version	5.6.31-0ubuntu0.15.10.1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `CloudasaService_material`
--

DROP TABLE IF EXISTS `CloudasaService_material`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `CloudasaService_material` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(50) NOT NULL,
  `title_content` longtext,
  `title_date` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `CloudasaService_material`
--

LOCK TABLES `CloudasaService_material` WRITE;
/*!40000 ALTER TABLE `CloudasaService_material` DISABLE KEYS */;
INSERT INTO `CloudasaService_material` VALUES (1,'compute','Virtualisation of the compute platform is commonly achieved through software virtualisation, which allows for stateless commodity hardware to be used, which in turn reduces complexity and cost. By virtualising the physical compute platform, the physical characteristics of the compute platform are detached, making it possible for the virtual machines running on the physical platform to move between platforms seamlessly. The key areas of compute virtualisation to focus attention on are the Processor, the Hypervisor (or Virtual Machine Monitor) and Guest O/S.\r\n\r\nBoth Intel and AMD provide enhancements that improve the performance and security of the hypervisor running on X86 CPU architecture. These enhancements were needed in order to meet the requirements of virtualisation, and eliminate the emulation penalties incurred by hypervisors hosting multiple operating systems. Both Intel-VT and AMD-V processors support these enhancements that are essential for efficient compute virtualisation.\r\n\r\nThe overhead of virtualisation on the hypervisor can be reduced by employing hardware assisted virtualisation on the processor. Intel and AMD processors support the extensions needed for hardware assisted virtualisation, although this will increase the overhead on the CPU. In essence, hardware assisted virtualisation moves the virtualisation function from the software running on the physical server, to the processor.\r\n\r\nFull software virtualisation (for example VMware or Hyper-V) runs at the software layer and requires no modifications to the hardware platform. Full virtualisation will run any unmodified guest O/S, which makes it a popular, and requires no special hardware. Using traditional software virtualisation also allows for existing hardware platforms to be used, which is an attractive proposition for businesses that have already made a large investment in compute platforms that have not been fully depreciated.\r\n\r\nParavirtualisation (for example Xen and KVM) is a lightweight virtualisation mechanism that requires the O/S running on the compute platform to be modified. This will introduce some supportability challenges, but does reduce the overhead on the processor.\r\n\r\nThe fundamental benefit of virtualising the compute platform is to improve utilisation of the compute resource through multi-tenancy on the hypervisor. Virtualisation allows for the sharing of the compute platform by assigning virtual CPU.s to the resident VMs. So, a dual quad core processor on a physical server will provide eight vCPU.s to be shared by the virtual machines resident on that physical server.\r\n\r\nSome examples of the versatility that compute virtualisation introduces are:\r\n\r\n. Migration of VMs between physical servers \r\n . High availability for VMs through failover mechanisms \r\n . Resource scheduling to move VMs to physical platforms when utilisation dictates \r\n . Fault tolerance by providing multiple instances of VM images','2017-05-09 18:54:36.000000'),(2,'datacenter','Datacenters are continually under tremendous strain as a result of the considerable demands placed upon them. Storage is a major contributor to the depletion of datacentre resource, as is the ever expanding need for compute. The only feasible approach to supporting the growth of the IT infrastructure needed to support a growing business is to dramatically improve efficiencies. This is achieved through consolidation and of course the virtualisation of the cloud building blocks. By virtualising the storage and compute infrastructure, densities are increased, and the datacentre footprint needed is reduced, when compared to what was once considered traditional computing techniques, before virtualisation mechanisms had fully evolved.\r\n\r\nDatacentre managers rely on forecast data, provided to them by infrastructure owners, showing any planned infrastructure changes and growth. The infrastructure owners in turn rely on forecasts of growth and development from the business technology teams. These forecasts are typically very hard to maintain accurately, with unstable markets and business uncertainties, and so the datacenter manager cannot be confident that the information needed will be delivered consistently, or indeed materialise at all. Some of the information needed by the datacenter managers can, to a certain degree, be addressed through trend analysis, but this will not provide an accurate picture of the future infrastructure needs, and it can be dangerous to rely on trend analysis to attempt accurate forecasting.\r\n\r\nThe Cloud management, automation and orchestration tools that are deployed in a well designed Cloud infrastructure will provide access to accurate data that can be used for trend analysis, and capacity reporting. This data will be used to provide regular updates on capacity versus utilisation, which will help tremendously in justifying the datacentre costs. It will also provide the business teams with accurate chargeback data, and provide visibility for all teams of the efficiencies of the datacentre.','2017-05-09 18:55:13.000000'),(3,'data','It is common knowledge that the rate of data growth is exponential, globally. This growth is relentless, and affects every type of business, large and small, and every business that generates data must contain this growth regardless of any physical infrastructure limitations. The problem will not go away.\r\n\r\nThe technology used to store data has advanced considerably in the past 20 years, and this technological advancement has undoubtedly helped contain data growth over this period, but only up to a point. For example, there are performance consequences to simply increasing the density of, say, a physical disk drive, and so in recent years it has been the role of storage virtualisation to improve efficiencies and increase capacities in storage arrays whilst maintaining performance. Until fairly recently it was commonplace for datacentres to be consumed by fully populated storage arrays and frames that may have been only 25% utilised, which represents a large amount of physical storage serving no purpose in a large estate. These inefficiencies may have been palatable 10 years ago, but are simply unacceptable today for a whole host of reasons, not least of which being cost.\r\n\r\nThe different types of data introduce additional storage challenges. The creation of unstructured data, for example, and the ability to identify the storage and retention needs for this data, perhaps for compliance, requires modern thinking and modern mechanisms. Both structured and un-structured data will benefit from virtualisation at the storage layer through the adoption of thin pools of storage and data de-duplication, but we are still left with the need to understand what data exists, and in some detail, before we can be confident that we are managing that data estate effectively. This requires additional tools that have the intelligence built in to help recognise data and report effectively on the data types. There are many compliance challenges that need to be addressed when dealing with unstructured data, and this all comes back to the need to know what data you have stored. \r\n\r\nCloud is driven by virtualisation across all of its building blocks. It follows therefore that all of the technology exists in Cloud, today, to contain data growth as effectively as is currently possible.','2017-05-09 18:55:41.000000'),(4,'globalisation','The goal for globalisation in IT is to spread roles and responsibilities beyond the local region where they may have existed since inception. This allows for fewer regional \'globalised\' centers to exist, supporting key functions such as the help desk, software development, change and problem management, to name only a few.\r\n\r\nThe automation of the routine tasks is needed to globalise effectively. For example, a routine request may be received from an IT service user for a change to be made to their application backup schedule. This task can be easily transformed from the manual task that uses the vendor backup software interface, to an automated task that is menu driven. This is the case for many other routine tasks, such as requesting a new service from a service catalogue (compute, storage, database instance), or raising a change request, and all of these can be automated using automation tools, component tools or a combination of both.','2017-05-09 18:56:59.000000'),(5,'intro','Cloud-as-a-Service.co.uk is a site devoted to Cloud services. We are ourselves a service, in that we can assist you with your Cloud strategy, and will help explain what Cloud really means - the benefits, the pitfalls, and how the principles of Cloud can benefit any business, large or small. On close inspection, Cloud is a complex array of choices. Some are compelling, all are confusing.\r\n\r\nThe term \'Cloud\' reflects the access and delivery characteristic for the service, be it platform, storage, software, or some other service. The infrastructure design that effectively supports the notion of a cloud service is still essentially a collection of IT assets delivering an IT service. A rack of hardware in a datacenter somewhere if you like. The real secret to Cloud, however, lies in the IT infrastructure\'s ability to meet the five tenets of Cloud, which is achieved through the virtualisation of each infrastructure building block: Compute, Storage and Network. It is virtualisation that makes any Cloud service possible. The Cloud tenets are explored in a little more detail in the next section, ‘What is Cloud?’.\r\n\r\nCloud-as-a-Service will explain the service models in some detail, address the cost of Cloud services, and how the capability of the modern Cloud infrastructure is built to abstract the physical infrastructure from the component service or services being delivered to the end user.\r\n\r\nSo, before we delve in to the detail, let\'s make one thing clear - a Cloud service does not necessarily need to be supplied by a remote third party service provider, such as Amazon or Microsoft. The very same principles that define Cloud can be built on your own premises, and this is an important facet. A Cloud service is a set of principles, and these principles can be built in to an on-premise (private) Cloud infrastructure, and will already exist as a framework for all off-premise (public) Cloud instances delivered by a third party. This is the highest level differentiator: on premise Cloud, that you own and manage, or Public Cloud, that someone else owns and manages remotely for you. Both are equally viable and have their own merits.\r\n\r\nThis leads nicely to the definition of Hybrid Cloud, which allows a business to combine both public and private Cloud services in to an IT strategy. \r\n\r\nWhen exploring the cost of public and private Cloud, it becomes apparent that both are compelling for different reasons – for example, Public Cloud has high OpEx but lower support overheads, and Private Cloud has higher CapEx and provides greater ownership and control over data (PCI, GDPR are examples of drivers towards or away from an on-premise instance of Cloud computing). To make the decision even more challenging, the cost for an effective private Cloud is reducing, as technology moves forward significantly, reducing the infrastructure complexities that exist in legacy architectures. You see, it\'s not so straightforward!','2017-05-09 18:57:44.000000'),(6,'provisioning','For Cloud to meet the Self Service tenet, there needs to be a portal interface that provides the end user with the option to select all services and resources that they need, and to submit that form in the knowledge that the request will complete promptly. For this to be achievable, there needs to be automation and orchestration integrated in to the Cloud framework, to provide an agile and efficient service delivery to the end user. When you consider that the service provider will likely have multiple physical platforms, from multiple vendors, ensuring that each is capable of coordinate the provisioning of its resource with the next platform (for example, provisioning an amount of block storage for a specific virtual machine) is non-trivial. A well designed service catalogue is needed as a starting point, and the orchestration between each service catalogue item developed. This is an essential feature that is needed for the end user to be offered a self-service mechanism to request new resources or increase the amount of resource it currently has access to. This removes the considerable delay that is experienced through traditional service request mechanisms, and the management overhead that comes with that.\r\n\r\nOnce again, it is no surprise that virtualisation is a key feature in making it possible to design an effective self-service portal. It is however the combination of component management tools and automation tools combined that make self-service possible. Despite there being many tools available to help deliver automation, making the final self-service solution work effectively depends on the right combination of tools for the platforms that are delivering the service to the end user. This can be achieved, and commonly is achieved by existing service providers, through OpenStack. OpenStack provides services to manage the automation across the various components of compute, storage and network.','2017-05-09 18:58:15.000000'),(7,'network','Network virtualisation has been available for some time in the form of VLANs, which segment resources in to subnets. Workloads and trust groups can be segmented based on business unit, application type, data sensitivity and performance.\r\n\r\nWANs also support virtualisation by having services run in one geographic location connected to another by dark fiber.\r\n\r\nFor Storage Area Networks (SAN), fully converged connectivity that employs Fibre Channel over Ethernet (FCoE) to the storage removes the need for additional high cost infrastructure, such as SAN switches and HBA cards on the hosts. However, FCoE has not taken off as was anticipated some years ago, and not all storage vendors support FCoE on their enterprise platforms. iSCSI is another protocol that allows for the convergence of storage and network, and it a realistic alternative to the higher cost SAN. Convergence also introduces some management challenges between Network and Storage support teams, and so ownership of functions and responsibilities needs to be clearly defined. But the fact remains that full convergence is not as popular among infrastructure users as you would expect it to be based on the compelling features it delivers. This may be because a considerable investment may have already been made in a SAN infrastructure, or a shift towards NAS as a lower cost alternative to SAN now that the network can support far higher speeds.\r\n\r\nIn a virtual datacenter, networks can be formed from discreet segments, on which traffic can be isolated, and these can be managed as logical networks. 10Gbps Ethernet will support distances of 300 meters on multimode fiber and over 40km on single-mode fiber, and has evolved such that the latest Ethernet supports all upper layer services.\r\n\r\nVLANs are switched networks that are logically segmented regardless of the user\'s physical location, and can be created at layer 2 of the OSI model. Similar to the way in which SAN switches can be segmented in to virtual SANs (VSANs), with traffic limited to the VSAN, a network switch can be divided in to multiple VLANs. In both cases, broadcast traffic is reduced and more bandwidth made available for user traffic. Routers are used to connect separate networks that are segmented in to subnets, or to access remote sites across the wide area links. Where you have multiple VLANs on a switch, it is necessary to enable VLAN Tagging on the ports that communicate out. When VLAN Tagging is enabled, a tag is inserted in to the Ethernet frame that tells the switch which network the packet is destined for. Ports that are configured for VLAN tagging are called trunk ports. A common use of VLAN Tagging are Inter Switch Links (ISLs), which are common in SAN fabrics and used to communicate packets between switches. So, if a host is connected to a network switch that is configured for multiple VLANs, the network interface on the host (NIC) can function as if it were multiple NICs on different physical networks. This is particularly useful for virtual machines\r\n\r\nThe association of Virtualisation with Cloud means that you will find the word ‘Cloud’ wrapped up in numerous new products. Call me cynical, but it is what senior management want to see, and means that adopting a new product or platform that has ‘Cloud’ in the title will be well received when it comes to justifying the spend. Taking my cynical hat off for a moment, there are valid cases, such as Cisco VACS (Virtual Application Cloud Segmentation) which applies the culture of Cloud (self service for example) to the provisioning of networks.','2017-05-09 18:58:57.000000'),(8,'performance','Virtualisation increases efficiencies considerably. Take storage for example. Gone are the days where storage provisioned to a server was pinned to a particular tier, and in many cases that single tier would have an entire storage frame dedicated to it. When you also consider that thin provisioning may not have been supported at that time in many enterprise class production storage arrays, it was a time of great inefficiencies.\r\n\r\nOne feature available today through virtualisation is the policy based movement of chunks of data between higher and lower performing tiers of physical storage within the same frame. This movement of data will be based on the I/O profile of the data chunk, and can be closely controlled through policy definitions. This automated movement of data allows for at least three tiers of physical drive to exist within the storage frame, whilst maintaining high storage I/O performance. This form of virtualisation increases the density of the storage array, perhaps even doubling the capacity within the same physical footprint. All of this is achieved whilst at the same time improving the service to the end user.\r\n\r\nWe will of course reach a point in the near future where it becomes financially viable to have a storage array fully populated with Solid State drives, but at present it is far more efficient to increase the amount of the high capacity lower cost SATA or NL-SAS drives and maintain a relatively small amount of SSDs. The automated tiering mechanisms make this possible.','2017-05-09 19:00:57.000000'),(9,'advantage','The Cloud service model provides a technology framework that removes much of the frustration that exists with the legacy approach to the provision of IT services. As we have seen in the description of Cloud, the infrastructure must be highly efficient in order to deliver the tenets that define Cloud.\r\n\r\nSo let’s take a look at a fairly typical ‘legacy’ infrastructure scenario:– \r\n\r\nA new compute platform is needed to support a production application that has been developed to run ‘in-house’. The steps that would be followed in the legacy infrastructure scenario would look something like:\r\n\r\n1. Establish the compute characteristics needed to support the new application \r\n\r\n2. Raise a request to the IT infrastructure team for a new compute platform (1 week) \r\n\r\n3. IT Infrastructure team identifies platform from their service catalogue \r\n\r\n4. Purchase order is raised for the new physical compute platform, and approval requested from a list of approvers (2 weeks) \r\n\r\n5. On approval, a purchase order is raised by the purchasing team and the order dispatched to vendor or IT reseller \r\n\r\n6. Lead time for delivery established (8 weeks) \r\n\r\n7. Request raised storage (assuming SAN or NAS) (2 weeks) \r\n\r\n8. On delivery of the server hardware, the IT infrastructure team will build the new platform and make ready for use \r\n\r\n9. Request network connectivity (1 week) \r\n\r\n8. Application build and go live (2 weeks)\r\n\r\nThis \'legacy\' example has taken 4 months from start to finish. It may seem an extreme example, but it is not. I can assure you that this process for the provisioning of a new environment is still standard practice in many large organisations.\r\n\r\nThe same series of steps for a Cloud service, assuming that the requester knows what compute platform to order, can be completed within a matter of a days. The porting of the application and migration of data will of course need to be planned effectively, and the correct Cloud service requested (for example, Infrastructure as a Service), but the actual procurement of the compute platform itself (IaaS), which took more than three months in the legacy scenario, is completed within a matter of minutes! \r\n\r\nOne obvious challenge that remains despite the infrastructure efficiencies is the movement of data between premise and Cloud. This will remain one of the steps that requires some additional effort (depending of course on the amount of data), but is still achievable with relative ease. There are a myriad of Cloud mechanisms that allow for a vast range of functionality. One of which being the ability to build containers that hold application environments, and these containers can move between Private and Public Cloud platforms.\r\n\r\nOne of the clear objectives of a Cloud service delivery mechanism is that all of the provisioning requests that exist in a traditional new environment build workflow can be automated, and delivered in a fraction of the time. This gives any business or LOB within an enterprise a clear competitive advantage when responding to market forces or business drivers.\r\n\r\nThe building of a Private, on-premise Cloud has been made considerably easier in recent years. This is because innovation within IT as a whole is shifting away from proprietary physical infrastructure, such as the monolithic storage arrays and appliances that have ‘intelligence’ built in to the vendor’s architecture, towards a software defined approach to building scalable platforms. This software defined approach to storage and compute has spawned a (relatively) new wave of highly scalable IT models that are built on commodity hardware. This lowers the cost considerably, as you are no longer investing in very high cost proprietary physical infrastructure, but instead investing in a software solution that will provide the framework to build (or scale) out across many standard server and storage type platforms. This IT model is broadly referred to as HyperConverged Infrastructure (HCI). HCI delivers very high performance, and tremendous degrees of scalability, and will typically be accompanied by a management interface that allow for speedy provisioning for new environments. Collectively, these HCI features satisfy the tenets of Cloud. \r\n\r\nHaving an on-premise instance of Cloud can help with regulatory and compliance type restrictions on where data can be stored, ensuring that you maintain control of your infrastructure. This can in turn reduce the impact from such things as a third party service outage that may be the result of mismanagement of infrastructure or resource. Or a security breach on a service providers infrastructure.','2017-05-09 19:01:24.000000'),(10,'costs','With the highest utilisation being driven by virtualisation, and the consolidation of resource on to fewer physical units, costs overall will inevitably reduce. This includes a reduction in the support and management run rate through the introduction of automation and orchestration. The Cloud infrastructure design that is based on virtualisation introduces a far more flexible approach to support. It is possible through this design to move roles to regions that are remote to the infrastructure, which allows for consolidation of teams and regional functions. All of this counts towards the reduction in costs. As we have demonstrated in the previous sections, a virtualised infrastructure results in considerable efficiencies, and no longer are there large pools of unused resource. And the tools introduced to run the Cloud will reduce the support overheads needed to monitor, alert, and report.\r\n\r\nThe impact of infrastructure outages and faults on the business functions is also reduced, and in some cases removed completely. This allows, in many cases, for the business units to continue functioning without an interruption to the service delivered by IT. This is achieved through the consequences of virtualisation, which make it very easy to move a process from one platform or physical location to another, in many cases seamlessly. This is also the case for storage and network, and so there is no reason why a business process cannot be \'live\' in two locations at the same time, making failover in the event of an outage or catastrophe seamless to the end users. The business continuity model described here allows for two production sites to exist, and so utilise the resource that was once sitting idle waiting for disaster recovery to be invoked.\r\n\r\nBy adopting a cloud infrastructure approach to support business functions, it is possible to move away from the business aligned high CAPEX approach to purchasing IT resources for individual lines of business, towards a consolidated and highly utilised shared pool of resources.','2017-05-09 19:01:55.000000'),(11,'scalability','With the introduction of what is effectively an abstracted layer of virtualisation for each Cloud building block, it is possible to have total control over what resource runs where, and how it runs. So, the removal of the traditional, or legacy pools of resource dedicated to a particular business function, and the introduction of the single virtual resource layer across each building block, means that resources can be moved seamlessly between physical platforms.\r\n\r\nThe abstraction from the physical layer also allows for physical resources to be increased without risking interruption to the business functions and processes using them. This makes for a scalable infrastructure. It should no longer matter to the end user where their process runs, or where their data resides, as long as the agreed service levels are met. The physical infrastructure behind the service delivery can therefore be high-end bespoke, or commodity, and that flexibility gives the infrastructure owners far greater choice on what physical platform to invest in.\r\n\r\nAn effective Cloud architecture allows for resources to be managed closely. There are a myriad of tools that can be used to manage the Cloud, and the decision on which combination of tools to use will depend to a certain degree on the chosen vendor physical platforms. Whichever combination of platforms are used, the management tools available (for example, vCloud and vCenter suites, Smarts, ProSphere, Watch4Net) will provide a high degree of visibility of resource utilisation, and the utilisation data will be accessible to report and alert on. This can trigger automated virtual pool expansion, or the process needed to request additional physical resource.\r\n\r\nResource increase is therefore a seamless operation made possible by the flexibility that Cloud, through virtualisation, introduces.','2017-05-09 19:02:18.000000'),(12,'manageability','The ability of a physical Cloud infrastructure to fulfil service requests relies on the ability of the component (building block) tools to integrate with each other, and also with the automation and orchestration tools. These tools must have the capability of taking requests from the enterprise request mechanisms (perhaps BMC Remedy or similar in a private Cloud scenario), updating the workflow as requests are satisfied, and maintaining the enterprise CMDB without the need for manual intervention. This is not an easy construct to architect, but most tools will interface with the other tools in the management framework to dramatically simplify the Cloud service request process.\r\n\r\nManaging a Cloud infrastructure using this style of high level framework will change the way infrastructure support teams work. It will make globalisation more easily achievable by simplifying the routine management roles, allowing the technicians to perform more of a low level technical function, and having the tools automate the high level tasks. The handover of support between regions in a \'follow-the-sun\' support model depends a little less on interaction between the teams with a simplified role structure. This in turn removes the risks that are inherent when geographically remote teams are required to work together effectively.','2017-05-09 19:02:58.000000'),(13,'storage','We have touched on much of this already, but to re-iterate, storage virtualisation has been an essential goal for all storage vendors, and is essential in the challenge of supporting the exponential growth in demand for storage. These features are possible across both fibre channel SAN storage, and Network Attached Storage (NAS), and achieved through a number of key features:\r\n\r\n. Thin Provisioning \r\n. Automated Tiering \r\n. De-duplication\r\n. Data compression\r\n. Virtualisation Controller \r\n. Virtualisation API\'s \r\n\r\nThin Provisioning allows for significantly higher storage utilisation, eliminating as much as possible the need for large expensive pools of unused storage. This is now achieved by all vendor platforms, and an essential virtualisation feature (see the virtualisation summary at the top of this page). Some vendors were surprisingly slow to engineer thin provisioning in to their high-end enterprise platforms. EMC for example were without thin provisioning on their block storage platforms until they introduced the VMAX storage array in about 2010 (2011 onwards by the time the VMAX reached some customers). By comparison, 3PAR introduced their array with thin provisioning in 2005! I guess EMC were perhaps a little concerned about their potential for lost revenues with these new efficiencies, so were not so keen to develop them (there I go again with the cynicism).\r\n\r\nAutomated Tiering is the ability to support high and low performing storage tiers, perhaps in the same array (but not necessarily), and for data to be seamlessly moved to the appropriate tier depending on the I/O profile of the data residing on that portion of disk. The higher performing tiers will be more expensive, and so unnecessary consumption will be kept to a minimum, while the lower cost and performing tier will be grown to support the majority of data. Automated tiering allows for a far higher density of low cost storage whilst maintaining I/O performance for the applications that need it. Multiple tiers can reside in the same storage frame, or can in some cases reside in separate frames and be tiered by an external virtualisation controller.\r\n\r\nVirtualisation Controllers provide an abstraction layer between physical storage and host platforms. This allows for data to be moved between storage platforms and between physical locations without interrupting the service, and in most cases without the host server and application being aware. An abstracted virtualisation layer also provides enhanced capabilities that allow, for example, applications to write to two physical datacenter locations simultaneously (cross site mirroring), which makes for seamless application failover in the event of a localised disaster. Virtualisation controllers can also perform some of the functions that are internal to many storage arrays, such as automated tiering between physical storage arrays.\r\n\r\nStorage Virtualisation APIs are available for hypervisors to transfer storage workload to the storage array, reducing the overhead on the hypervisor and thus improving efficiency and performance. Some examples of storage workload that can be transferred to the storage array (known as primitives by a popular hypervisor), are:\r\n\r\n. XCOPY \r\n. Block Zeroing \r\n. Write Same (zero) \r\n. Hardware Lock Assist \r\n. Atomic Test and Set (ATS) \r\n. This Provisioning Reclaim (UNMAP)\r\n\r\nBut once again, the latest software defined approach to storage is shifting the focus away from the very expensive proprietary physical platforms, and on to the commodity driven platforms that you can pull off the shelf and use. These SDS platforms scale to such a size that it makes the competing proprietary platform appear restrictive.','2017-05-09 19:03:31.000000'),(14,'support','If we were to chart the evolution in IT services delivery over the past 10 years, we would see a shift, fairly slow in some industries, from the traditional physical compute platforms on to virtual. The entirely virtual datacentre will have far fewer physical components than the datacentre required to support the same service of 10 years ago, before large scale virtualisation.\r\n\r\nThis adoption of virtualisation comes with it a new approach to support, and the introduction of mechanisms that have been around for many years, but only maturing fairly recently. This virtualised environment, if well designed, will be easier to manage. This will be so for all aspects of support, such as BAU operational support, monitoring and alerting, reporting, performance management, change and problem management etc.\r\n\r\nAlthough the combination of tools needed to fully automate is fairly complex, they do at least exist in a form that can be combines to deliver automated mechanisms for many routine tasks. When automation has been implemented successfully, many functional roles that today require many skilled technicians to support, can be procedurised and even offshored to remote teams. This change and relocation of roles within the regions does not need to result in large scale upset for the existing support staff, but should be seen as offering an opportunity to develop the roles and skills that remain.','2017-05-09 19:04:07.000000'),(15,'businesscontinuity','With the significantly enhanced compute and storage mobility that comes with virtualisation, applications can run in multiple locations, and data written to those locations through the storage virtualisation layer. This can result in seamless failover for applications, and seamless movement of applications between physical platforms. \r\n\r\nMany industries are required to test their business continuity capability, and demonstrate to the regultors that they can maintain their critical applications following a disaster in the production datacenter, or in the location where the production application is running. These formal tests can be disruptive and require application downtime, which is not viable for many applications that run within a global enterprise. This continues to be a significant challenge for legacy applications today, that have not been built on a virtualised infrastructure. By moving to a Cloud service delivery, all of the mobility and availability features will already exist, and proving business continuity capabilities will be achievable without any noticeable interruption to the service.\r\n\r\nMeeting Business Continuity demands is therefore achieved by default as part of the features of Cloud service delivery.','2017-05-09 19:04:42.000000'),(16,'history','Virtualisation is a mechanism by which access to a resource is manipulated by a layer that separates access to the resource from the physical layer. Virtualisation is commonly described as being an abstraction layer between a physical resource and the end user. An easy example of compute virtualisation is the Hypervisor (VMware or Hyper-V for example).  A Hypervisor runs on top of an operating system such as Linux, and is used to present multiple virtual environments to an end user. So a single Linux server can appear as, say, 20 Windows servers to the outside world. It is not hard to see the benefits of this.\r\n\r\nVirtualisation of the compute platform dates back to the mid 1960\'s, when IBM introduced an early version in the form of the IBM System 360-67 Mainframe. The mini computers that evolved in the 1970\'s, most notably the Digital Equipment Corporation VAX 11/780, very successfully incorporated a virtual memory system, and became a market leader in mini computing. The VAX 11/780 and subsequent iterations of the mini that ran VMS and OpenVMS relied \r\n\r\nVirtualisation on x86 appeared in the late 1990\'s when a research team at Stanford developed VMware, code named DISCO. VMWare and other compute virtualisation platforms entered circulation among enterprise users in early to mid 2000, and revolutionised the strategy for compute processing from that point. It was then a race to adopt a virtualised compute strategy to squeeze ever greater efficiencies from physical hardware. Before virtualisation took hold, the standard approach for all compute was to purchase dedicated physical machines. This resulted in a huge spread of servers running individual business applications. This model is by far the most inefficient way to run an IT infrastructure. When you consider that utilisation may, at best, run at 35% on average, this means that something in the region of 65% - 70% of your expensive physical resource is wasted. It is sitting there on the off chance that it may be used as demand grows. The inefficiencies become even more alarming when you factor in the additional overhead of disaster recovery, which may see you running a mirror site which is sitting idle waiting for that disaster to happen.\r\n\r\nThe benefits of virtualisation were also realised, albeit later, for data storage. For most of the first decade since the year 2000, storage suffered the same pre-virtualisation inefficiencies. This was a time when the SAN (Storage Area Network) was growing as an enterprise storage infrastructure. The SAN allowed for the consolidation of storage by providing a fibre channel network for servers to attach to storage (block storage LUNs) presented from a ‘farm’ of storage arrays. The SAN provided a high speed storage platform, similar in performance as fast disks attached directly to the server. It reduced the management overhead by consolidating storage in to manageable storage array ‘farms’, rather than have hundreds of servers with direct attached physical disks. Although the SAN improved storage efficiencies (can you imagine the overwhelming challenge of managing hundreds of servers with at least 4 physical disks each?), the earlier storage arrays (with a few exceptions) did not support virtualised mechanisms like thin provisioning, de-duplication and compression, and so the same 65% - 70% wastage of storage existed across the infrastructure. 3Par (now owned by HP) was an early advocate of thin provisioning and built its popularity on being able to ‘over-provision’, and therefore drastically improve storage utilisation efficiencies. A Thin Provisioned LUN (disk drive) is essentially a container that consumes no physical storage until it is written to, and will only consume the amount of storage written to it. Compare that to the pre-virtualisation LUN, which will have reserved (effectively consumed) the size of the LUN from the moment it was created. \r\n\r\nAnd now we enter the world of Software Defined computing, which will incorporate all the current virtualisation mechanisms. These include a hypervisor, storage thin provisioning, de-duplication, and in addition will distribute workloads further by spreading the storage across many physical ‘nodes’.','2017-05-09 19:05:08.000000'),(17,'description','As discussed in the introduction, Cloud is essentially an IT service that is delivered across a network, but it is also a mode of operation. As a potential user of a Cloud service, you have the choice to build your own Cloud service in your datacentre, that you own, or use a third party Cloud service provider and let them do the hard part.\r\n\r\nThe traditional, perhaps legacy (non-Cloud) mode of operation is built around a physical IT hardware infrastructure running from an end user’s own, or co-located datcenter somewhere. This legacy infrastructure will have evolved over time and will almost certainly lack the efficiency and flexibility that defines Cloud service delivery.\r\n\r\nSo, although a Cloud operating model can run from your own physical infrastructure in your own datacentre facility, the model that we are most familiar with is that served by a third party provider, such as Amazon Web Services, Microsoft Azure, or Google Cloud Platform. In other words, Cloud is more of a methodology, built on infrastructure efficiencies that are achieved through the virtualisation of each of the physical building blocks. It is the consequence of these efficiencies that allows for an operating model that supports the tenets of Cloud, and it is these tenets that makes Cloud what it is:\r\n\r\nSelf service - the ability to request an IT resource or service from an Internet (network) delivered portal style interface (GUI)\r\n\r\nRemote network access – delivery of the IT resource or service is across a network \r\n\r\nShared resource among Cloud tenants – the infrastructure components supporting the IT resource or service are shared with other Cloud users\r\n\r\nRapid elasticity – the physical IT infrastructure will scale effortlessly to support the growth in demand for services \r\n\r\nMeasured service - the IT service or resource has the ability to adapt to the demand on it, gaining additional resource as needed\r\n\r\nSo now that we have acknowledged that Cloud is a methodology and mode of operation, and that it can run from anywhere (so to speak), we can explore the categories of Cloud available to a business. \r\n\r\nThese categories are broadly referred to as Public, Private, Hybrid and Edge, and are easy to understand once the fundamentals of Cloud are taken in:\r\n\r\nPublic: a Cloud service delivered by a remote third party\r\n\r\nPrivate: also known as \'on-premise\', is built and owned by the Cloud user\r\n\r\nHybrid: a combination of Public and Private services (perhaps you want your key applications running on-premise, and your backups written to lower cost public Cloud)\r\n\r\nEdge: A mechanism to address some of the physical limitations in delivering services globally (latency – the speed of light is a constant!)','2017-06-16 11:27:00.000000'),(18,'difference','The infrastructure that supports a Cloud service must be highly efficient in order to deliver the tenets that define a Cloud service, and it is the virtualisation of each of the infrastructure building blocks that makes a Cloud service achievable. The network delivery characteristics of the service being delivered is one of the tenets defining Cloud, but it is the efficiencies built in to the architecture that define it further.\r\n\r\nExploring each of the tenets of Cloud separately, it is possible to see how the principles are achieved.\r\n\r\nSelf Service – the user of a Cloud service must be able to request a service offering using a ‘self service’ style mechanism. In other words, there must be a portal of some description that offers a point-and-click request mechanism for a Cloud resource or service offering. This service offering or resource must then be made immediately available to the requester. If we refer back to the high level legacy infrastructure definition used previously, the stark difference between a Cloud and Legacy service delivery can be clearly seen. For example, in the legacy scenario, a request for a new compute platform to run an application would require the following steps:\r\n\r\n1.	Establish the compute characteristics needed and raise a request in to IT infrastructure team for a new compute platform (1 week)\r\n2.	IT Infrastructure team identifies platform from their service catalogue\r\n3.	Purchase order is raised for the new compute platform, and approval requested from list of approvers (2 weeks)\r\n4.	On approval, purchase order raised by purchasing team and order dispatched to vendor\r\n5.	Lead time for delivery established (8 weeks)\r\n6.	On delivery of server, IT infrastructure build platform ready for use (1 week)\r\n7.	Application build and go live (2 weeks)\r\n\r\nThe legacy example above has taken almost 4 months from start to finish. It may look like an extreme example, but it is not. I can assure you that this is standard practice in large organisations!\r\n\r\nThe same series of steps on Cloud, assuming that the requester knows what compute platform to order, can completed within a matters of days. The actual procurement of the compute platform itself, which took more than three months in the legacy example, is complete within a matter of minutes.','2017-06-16 16:29:00.000000'),(19,'author','The author of CloudasaService.co.uk is me, Paul Young. I have worked in IT for more than 25 years, starting out supporting Digital VMS, which introduced virtualisation early on with it\'s VAX/VMS architecture and clustered file system. Looking back, this was innovation, pure and simple, and should have been the proving ground for much of what we now take for granted in platform and storage virtualisation. However it sadly passed away, to be replaced by the standard platform foundations that we see today in Linux and Windows.\r\n\r\nI moved in to the predictable areas of Unix and Linux administration, and settled in to storage as a speciality about 15 years ago. I have held senior roles at BNP Paribas, Bank of America Merrill Lynch, and Goldman Sachs, as a leader of regional teams, attempting where possible to introduce innovation. \r\n\r\nI am now an Infrastructure Architect, using my experience to evangelise (horribly pretentious word!) through the adoption of today\'s innovation in software defined architectures. I have seen the shift towards software defined platforms, and the traditional vendors rapidly scramble to keep in the race. What the hyper-converged approach provides is immense, and is already proving itself a  viable alternative to the bespoke and very expensive legacy approach to IT infrastructure. It\'s great stuff, it works, and will provide all of the future proofing and scalability needed.\r\n\r\nI am always available to discuss any aspect of this site, and can be contacted on +44 7912 406757. Thanks for reading!','2017-06-17 15:17:00.000000'),(20,'whobenefits','With the introduction of what is effectively an abstracted layer of virtualisation for each Cloud building block, it is possible to have total control over what resource runs where, and how it runs. So, the removal of the traditional, or legacy pools of resource dedicated to a particular business function, and the introduction of the single virtual resource layer across each building block, means that resources can be moved seamlessly between physical platforms.\r\n\r\nThe abstraction from the physical layer also allows for physical resources to be increased without risking interruption to the business functions and processes using them. This makes for a scalable infrastructure. It should no longer matter to the end user where their process runs, or where their data resides, as long as the agreed service levels are met. The physical infrastructure behind the service delivery can therefore be high-end bespoke, or commodity, and that flexibility gives the infrastructure owners far greater choice on what physical platform to invest in.\r\n\r\nAn effective Cloud architecture allows for resources to be managed closely. There are a myriad of tools that can be used to manage the Cloud, and the decision on which combination of tools to use will depend to a certain degree on the chosen vendor physical platforms. Whichever combination of platforms are used, the management tools available (for example, vCloud and vCenter suites, Smarts, ProSphere, Watch4Net) will provide a high degree of visibility of resource utilisation, and the utilisation data will be accessible to report and alert on. This can trigger automated virtual pool expansion, or the process needed to request additional physical resource.\r\n\r\nResource increase is therefore a seamless operation made possible by the flexibility that Cloud, through virtualisation, introduces.','2017-06-21 18:32:00.000000'),(21,'publicorprivate','If we were to focus purely on the cost of Public and Private Cloud platforms, then we would see that Public Cloud is not an inevitable first choice. The cost, for example, of running a database server capable of supporting in excess of 500 concurrent users, will most likely cost somewhere in excess of £5K per month. This is a very rough estimate to provide us with an example to work from, and this cost is not taking in to account the additional storage, network, web server and content server that may also be needed. So, you can see how the three year costs can mount up, and how these costs can go a long way towards the justification of an on-premise platform instead. If the proposed on-premise platform conformed to the tenets of Cloud, and the infrastructure build costs were lower over a three year period when compared with the equivalent public Cloud costs, then it would be natural to consider building your own Cloud. A private Cloud is obviously made more practical if a datacentre facility is already available. It is also made more practical if the technology existed to reduce the impact on the datacentre (cab space, power, heat dissipation), which in turn lowers the running costs for on-premise Cloud.\r\n\r\nUntil fairly recently, the private Cloud platform options were limited, and most enterprises looking to build their own Cloud were reliant on high cost proprietary vendor hardware. These proprietary platforms lacked the overall flexibility to effectively deliver the tenets needed to qualify as a Cloud platform without at least some complex orchestration tools, and the hardware that was available to attempt to deliver an on-premise Cloud infrastructure severely impacted the datacentre. This limitation and impact is now largely gone due to the very latest software defined techniques, and the choice of platform available today has grown significantly. And the choices continue to grow! \r\n\r\nYou can today build a genuine, cost effective private Cloud that delivers the same level of service that the Public equivalent delivers. And in turn you can continue to control and own your data, and be rest assured that your service will not be impacted by third party service limitations that are outside of your control. The overhead associated with a private Cloud, such as support costs, can be diluted significantly if a team already exists supporting existing platforms. \r\n\r\nThe Software Defined computing platforms that run on commodity hardware are very much simpler in their design, as they are not built on complex physical hardware, and so in turn do not need the specific skilled technicians needed to support them. In the legacy exampled used previously, the connectivity between compute and storage is highly complex. It can typically require server and compute virtualisation (hypervisor) skills, Storage Area Network skills (fibre channel networking, masking and zoning), and specific vendor storage skills (EMC, HDS, NetApp etc). These skills are understandably costly, as these architectures are themselves designed to deliver a Tier-1 service. In practice, none of this is needed in the latest software defined world of IT.\r\n\r\nYou would be excused in thinking that I am just a little fanatical about software defined computing (or Software Defined Storage, SDS, which is a collective term commonly used). This approach to IT introduces such high levels of versatility, which in turn allow for platforms to scale out (grow) massively. The building blocks that make up the platform are high density, and contain the compute, storage, network and hypervisor. Multiple building blocks make up the platform, and can be added with ease when additional resource is needed. The building blocks are connected to each other using a very high speed interconnect, and the data is distributed among the building blocks. I would like to mention some of the Software Defined vendors that deliver this functionality, but that would amount to favouritism! (although I do have my favourites).\r\n\r\nThis still leaves us without an answer to the question – which is right for your organisation? Public or Private? Well, the easy answer is both, but the correct answer will depend on an analysis of the factors mentioned above.','2017-06-21 18:35:00.000000'),(22,'Temp','Temp','2017-07-10 21:45:00.000000'),(23,'publicdis','Starting with Public Cloud, the most obvious disadvantage is cost. It is expensive, and the range of options is overwhelming, even for those who are well versed in Cloud. So, whichever way you approach a move towards Cloud, it will most likely need a substantial investment financially. \r\n\r\nYou can spend money on consultancy to help reduce the Cloud service costs at the design stage, or you can save that overhead and attempt to design and build your Cloud environments yourself. To do the latter requires a low level understanding of the environments that will move to Public Cloud. This will include a thorough understanding of all virtual environments and the communication between each of them (messaging, VLAN requirements, firewalls, etc), and of course any proprietary software that supports functionality. \r\n\r\nThe second clear disadvantage of Public Cloud is its complexity. The complex nature of Public Cloud reflects the vast growth in functionality, and the battle between Cloud service providers to deliver even greater functionality to take full advantage of the flexibility that cloud presents. You can perform any action on your data in Public Cloud today (and its clever stuff, and at times confusing) that provides for an extraordinary range of functions that would have seemed almost impossible to achieve only a few years ago. The ability to perform analytics using Map Reduce style mechanisms on your vast repository of data to discover and react to trends is exciting. And the containers that allow for environments to be migrated in to and out of public Cloud is another example of how the community of Cloud service providers has had to evolve, and evolve quite rapidly at that. So all of this complexity does deliver an extremely broad range of functionality, which in turn goes some way in justifying the complexity.\r\n\r\nThe last disadvantage (for now) is the risk of \'lock in\' with any of the third party service providers. There may be a rather frantic stampede to public Cloud by businesses today, driven in most cases by management, but if you are not in a position to extract your service from public Cloud at some point in the future, then you run a serious risk of being held ransom at a later date. There are, again, mechanisms and new software defined platforms that provide the flexibility needed to avoid this risk.','2017-07-20 18:53:00.000000'),(24,'privatedis','As described in the previous section, Private on-premise Cloud is becoming an even more viable option for businesses. One obvious disadvantage is that the infrastructure will need to be supported by technicians, and that can be costly if a support team does not already exist. In addition, a datacentre or similar will be needed, and so private Cloud will be a more attractive proposition for a business that already has a datacentre or at least a suitable comms room with equipment, and a technical team providing support.\r\n\r\nAnother technical disadvantage associated with owning your own Cloud is the overhead associated with disaster recovery and high availability services. Having an infrastructure that you own will typically include the need for a second platform at a second (DR) location, and a backup process that includes policies for backing up data to disk or tape, and managing the movement of data or tapes to and from an offsite location. This all requires effort, and that comes at a cost. One solution to both of these challenges may be to adopt a Public Cloud strategy for disaster recovery and backups. In other words, containerise your application environments to make them portable between private and public Cloud, and perform backups to low cost Cloud object storage or similar. Public Cloud can then be used to run your application if DR invocation were needed.\r\n\r\nThere are modern backup mechanisms that allow for entire environments to be backed up to public Cloud, and can be converted to run-time environments very quickly.\r\n\r\nAll the procedures that are needed to run an enterprise IT infrastructure are also needed to run on-premise Cloud. This includes the obvious need for monitoring and escalation for faults and issues that occur during production hours, and out of hours. The modern vendor will help facilitate some of these requirements, but that does not remove the need for a permanent technical presence within the IT structure to provide onsite support.','2017-07-20 18:54:00.000000'),(25,'services','Lets start with the obvious – what are the generic categories of service offered by all service providers? \r\n\r\nIaaS – Infrastructure as a Service. This is exactly as it suggests, an infrastructure component (compute, storage, network)\r\nSaaS – Software as a Service. Software that can be accessed remotely, such as Salesforce\r\nPaaS – Platform as a Service\r\n\r\nThe Cloud service providers are numerous. The top two are, not surprisingly, Amazon Web Services and Microsoft Azure. If we look a little more closely at these two (sorry Google and IBM), we will see that each have their own set of acronyms for the ‘products’ they offer. Very broadly speaking (I did mention complexity as one of the disadvantages of Public Cloud!), these can be separated in to the physical platforms that we may already be familiar with (compute, storage, and network), and the software that runs on top (database, additional software to support functionality). To understand the ‘products’, let’s say storage, then it is necessary to understand what each ‘product’ description actually means. It now becomes even more complex, as storage can be broadly categorised in to ‘tiers’ - block, file, object, backup and recovery, and archive. Block is the higher cost and typically higher performing tier of storage (I am using my words very carefully as this is a minefield, with higher network speeds on Ethernet using IP being compared with ‘lossless’ fibre channel protocol block storage making this discussion a little contentious) and suitable for certain types of data, requiring high performance. Block storage uses LUNs (disks) to store data, and typically requires a filesystem to manage blocks of I/O being written to the disk. The LUN is presented to a host across a fibre channel connection using SCSI to communicate with the disk, or an Ethernet connection using iSCSI (which encapsulates the SCSI commands on a TCP/IP protocol connection). Block storage is the highest performing tier, and the data written to block storage is referred to as ‘structured’. \r\n\r\nFile storage is fairly self-explanatory, and can be seen in all workplaces when a user connects to their desktop and opens a file. Data held on file storage is referred to ‘unstructured’. File storage uses NFS, SMB and CIFS protocols to access file ‘shares’ (folders), and is typically a medium performing tier of storage.\r\n\r\nObject storage on the other hand is more complex, being the lower cost option. Object storage is again used for specific types of data, and is low performing, and essentially a ‘cheap-and-deep’ tier of storage. Object storage stores data in objects (no kidding), and each object contains meta data describing the object. Because high performance is not a characteristic of Object storage, objects will consist of static data, and will be duplicated among other datacentre locations to provide very high resilience against data loss. The complexity continues when considering the type of storage that is being used at the physical layer. This can be either a hard drive, or a solid state drive (SDD). This discussion alone could take up the remainder of this site. \r\n\r\nTo fully understand the compute on offer from a service provider (Amazon refer to their Cloud compute it Elastic Cloud Compute, or EC2 for short, and Microsoft have Virtual Machines), you need to have an appreciation of the physical aspects of the virtual machine that is being provisioned. Remember, this is all virtualised, so a virtual machine will be provisioned from a physical server running a specific rating of CPU, with a finite number of physical cores. The virtual machine will then be configured with a count of virtual CPUs (vCPU), which are directly related to the physical cores on the server. The environment will be shared with others running on the same physical server. The pricing for compute will be based on the performance characteristics of the physical server, and there will be varying levels of performance available, with the lower performing platforms being at a lower price point. It is therefore essential that the performance profile of the application moving to a Cloud compute platform is understood.','2017-07-21 18:37:00.000000');
/*!40000 ALTER TABLE `CloudasaService_material` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_group`
--

DROP TABLE IF EXISTS `auth_group`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_group` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(80) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group`
--

LOCK TABLES `auth_group` WRITE;
/*!40000 ALTER TABLE `auth_group` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_group_permissions`
--

DROP TABLE IF EXISTS `auth_group_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_group_permissions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `group_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_group_permissions_group_id_0cd325b0_uniq` (`group_id`,`permission_id`),
  KEY `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` (`permission_id`),
  CONSTRAINT `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `auth_group_permissions_group_id_b120cbf9_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group_permissions`
--

LOCK TABLES `auth_group_permissions` WRITE;
/*!40000 ALTER TABLE `auth_group_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_permission`
--

DROP TABLE IF EXISTS `auth_permission`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_permission` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL,
  `content_type_id` int(11) NOT NULL,
  `codename` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_permission_content_type_id_01ab375a_uniq` (`content_type_id`,`codename`),
  CONSTRAINT `auth_permissi_content_type_id_2f476e4b_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=22 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_permission`
--

LOCK TABLES `auth_permission` WRITE;
/*!40000 ALTER TABLE `auth_permission` DISABLE KEYS */;
INSERT INTO `auth_permission` VALUES (1,'Can add log entry',1,'add_logentry'),(2,'Can change log entry',1,'change_logentry'),(3,'Can delete log entry',1,'delete_logentry'),(4,'Can add permission',2,'add_permission'),(5,'Can change permission',2,'change_permission'),(6,'Can delete permission',2,'delete_permission'),(7,'Can add group',3,'add_group'),(8,'Can change group',3,'change_group'),(9,'Can delete group',3,'delete_group'),(10,'Can add user',4,'add_user'),(11,'Can change user',4,'change_user'),(12,'Can delete user',4,'delete_user'),(13,'Can add content type',5,'add_contenttype'),(14,'Can change content type',5,'change_contenttype'),(15,'Can delete content type',5,'delete_contenttype'),(16,'Can add session',6,'add_session'),(17,'Can change session',6,'change_session'),(18,'Can delete session',6,'delete_session'),(19,'Can add material',7,'add_material'),(20,'Can change material',7,'change_material'),(21,'Can delete material',7,'delete_material');
/*!40000 ALTER TABLE `auth_permission` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user`
--

DROP TABLE IF EXISTS `auth_user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `password` varchar(128) NOT NULL,
  `last_login` datetime(6) DEFAULT NULL,
  `is_superuser` tinyint(1) NOT NULL,
  `username` varchar(30) NOT NULL,
  `first_name` varchar(30) NOT NULL,
  `last_name` varchar(30) NOT NULL,
  `email` varchar(254) NOT NULL,
  `is_staff` tinyint(1) NOT NULL,
  `is_active` tinyint(1) NOT NULL,
  `date_joined` datetime(6) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user`
--

LOCK TABLES `auth_user` WRITE;
/*!40000 ALTER TABLE `auth_user` DISABLE KEYS */;
INSERT INTO `auth_user` VALUES (1,'pbkdf2_sha256$24000$p6hYvFRxE1x2$r+jCXDlDlcSgs48dQUMX4pRvoB0Dofb3bSvpSPNCltQ=','2017-08-04 10:37:51.990427',1,'admin','','','admin@cloudasaservice.co.uk',1,1,'2017-05-03 17:08:39.616947');
/*!40000 ALTER TABLE `auth_user` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user_groups`
--

DROP TABLE IF EXISTS `auth_user_groups`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user_groups` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) NOT NULL,
  `group_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_user_groups_user_id_94350c0c_uniq` (`user_id`,`group_id`),
  KEY `auth_user_groups_group_id_97559544_fk_auth_group_id` (`group_id`),
  CONSTRAINT `auth_user_groups_group_id_97559544_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`),
  CONSTRAINT `auth_user_groups_user_id_6a12ed8b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user_groups`
--

LOCK TABLES `auth_user_groups` WRITE;
/*!40000 ALTER TABLE `auth_user_groups` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_user_groups` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user_user_permissions`
--

DROP TABLE IF EXISTS `auth_user_user_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user_user_permissions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_user_user_permissions_user_id_14a6b632_uniq` (`user_id`,`permission_id`),
  KEY `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` (`permission_id`),
  CONSTRAINT `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `auth_user_user_permissions_user_id_a95ead1b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user_user_permissions`
--

LOCK TABLES `auth_user_user_permissions` WRITE;
/*!40000 ALTER TABLE `auth_user_user_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_user_user_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_admin_log`
--

DROP TABLE IF EXISTS `django_admin_log`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_admin_log` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `action_time` datetime(6) NOT NULL,
  `object_id` longtext,
  `object_repr` varchar(200) NOT NULL,
  `action_flag` smallint(5) unsigned NOT NULL,
  `change_message` longtext NOT NULL,
  `content_type_id` int(11) DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` (`content_type_id`),
  KEY `django_admin_log_user_id_c564eba6_fk_auth_user_id` (`user_id`),
  CONSTRAINT `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`),
  CONSTRAINT `django_admin_log_user_id_c564eba6_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=72 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_admin_log`
--

LOCK TABLES `django_admin_log` WRITE;
/*!40000 ALTER TABLE `django_admin_log` DISABLE KEYS */;
INSERT INTO `django_admin_log` VALUES (1,'2017-05-09 18:54:40.039353','1','compute',1,'Added.',7,1),(2,'2017-05-09 18:55:16.599105','2','datacenter',1,'Added.',7,1),(3,'2017-05-09 18:55:44.451103','3','data',1,'Added.',7,1),(4,'2017-05-09 18:57:02.372032','4','globalisation',1,'Added.',7,1),(5,'2017-05-09 18:57:47.400605','5','intro',1,'Added.',7,1),(6,'2017-05-09 18:58:18.574090','6','provisioning',1,'Added.',7,1),(7,'2017-05-09 19:00:19.735264','7','network',1,'Added.',7,1),(8,'2017-05-09 19:01:00.858458','8','performance',1,'Added.',7,1),(9,'2017-05-09 19:01:26.819552','9','advantage',1,'Added.',7,1),(10,'2017-05-09 19:01:58.250812','10','costs',1,'Added.',7,1),(11,'2017-05-09 19:02:21.201537','11','scalability',1,'Added.',7,1),(12,'2017-05-09 19:03:01.047588','12','manageability',1,'Added.',7,1),(13,'2017-05-09 19:03:34.116326','13','storage',1,'Added.',7,1),(14,'2017-05-09 19:04:10.566402','14','support',1,'Added.',7,1),(15,'2017-05-09 19:04:44.910181','15','businesscontinuity',1,'Added.',7,1),(16,'2017-05-09 19:05:11.218080','16','history',1,'Added.',7,1),(17,'2017-05-25 18:04:40.125615','5','intro',2,'Changed title_content.',7,1),(18,'2017-05-25 18:14:16.985050','5','intro',2,'Changed title_content.',7,1),(19,'2017-05-25 18:15:07.360094','3','data',2,'No fields changed.',7,1),(20,'2017-06-01 17:36:49.704180','5','intro',2,'Changed title_content.',7,1),(21,'2017-06-13 18:41:00.207113','5','intro',2,'Changed title_content.',7,1),(22,'2017-06-13 18:58:46.949513','5','intro',2,'Changed title_content.',7,1),(23,'2017-06-13 19:11:41.284922','5','intro',2,'Changed title_content.',7,1),(24,'2017-06-13 19:18:00.937798','5','intro',2,'Changed title_content.',7,1),(25,'2017-06-13 19:24:59.931517','5','intro',2,'Changed title_content.',7,1),(26,'2017-06-13 19:28:53.375178','5','intro',2,'Changed title_content.',7,1),(27,'2017-06-13 19:31:15.199412','5','intro',2,'Changed title_content.',7,1),(28,'2017-06-16 10:27:57.619122','17','description',1,'Added.',7,1),(29,'2017-06-16 15:29:20.557525','18','difference',1,'Added.',7,1),(30,'2017-06-17 14:24:51.997426','19','author',1,'Added.',7,1),(31,'2017-06-17 14:35:21.003815','19','author',2,'Changed title_content.',7,1),(32,'2017-06-20 14:57:31.867923','19','author',2,'Changed title_content.',7,1),(33,'2017-06-20 15:03:58.389564','19','author',2,'Changed title_content.',7,1),(34,'2017-06-20 15:06:02.815817','19','author',2,'Changed title_content.',7,1),(35,'2017-06-20 15:13:44.674012','17','description',2,'Changed title_content.',7,1),(36,'2017-06-20 15:16:52.561860','17','description',2,'Changed title_content.',7,1),(37,'2017-06-20 15:19:47.135515','17','description',2,'Changed title_content.',7,1),(38,'2017-06-20 17:20:07.088600','5','intro',2,'Changed title_content.',7,1),(39,'2017-06-21 17:22:27.001646','5','intro',2,'Changed title_content.',7,1),(40,'2017-06-21 17:23:14.237038','17','description',2,'Changed title_content.',7,1),(41,'2017-06-21 17:29:48.014968','9','advantage',2,'Changed title_content.',7,1),(42,'2017-06-21 17:30:27.783170','12','manageability',2,'Changed title_content.',7,1),(43,'2017-06-21 17:31:03.969842','9','advantage',2,'No fields changed.',7,1),(44,'2017-06-21 17:31:31.462146','11','scalability',2,'Changed title_content.',7,1),(45,'2017-06-21 17:32:36.578644','20','whobenefits',1,'Added.',7,1),(46,'2017-06-21 17:36:09.658343','21','publicorprivate',1,'Added.',7,1),(47,'2017-07-10 20:57:56.578340','22','Temp',1,'Added.',7,1),(48,'2017-07-15 18:44:50.550995','5','intro',2,'Changed title_content.',7,1),(49,'2017-07-15 18:46:56.058484','17','description',2,'Changed title_content.',7,1),(50,'2017-07-15 18:47:35.693330','9','advantage',2,'Changed title_content.',7,1),(51,'2017-07-15 18:57:07.824793','9','advantage',2,'Changed title_content.',7,1),(52,'2017-07-15 19:03:40.196269','9','advantage',2,'Changed title_content.',7,1),(53,'2017-07-15 19:19:38.185389','17','description',2,'Changed title_content.',7,1),(54,'2017-07-20 17:52:22.849647','9','advantage',2,'Changed title_content.',7,1),(55,'2017-07-20 17:53:52.378952','23','publicdis',1,'Added.',7,1),(56,'2017-07-20 17:54:33.778460','24','privatedis',1,'Added.',7,1),(57,'2017-07-20 18:22:16.242635','23','publicdis',2,'Changed title_content.',7,1),(58,'2017-07-21 17:36:01.628911','21','publicorprivate',2,'Changed title_content.',7,1),(59,'2017-07-21 17:37:33.471585','25','services',1,'Added.',7,1),(60,'2017-07-21 17:39:23.277295','6','provisioning',2,'Changed title_content.',7,1),(61,'2017-07-24 17:24:30.145166','16','history',2,'Changed title_content.',7,1),(62,'2017-07-24 17:26:17.327680','7','network',2,'Changed title_content.',7,1),(63,'2017-07-24 17:27:09.599390','13','storage',2,'Changed title_content.',7,1),(64,'2017-08-04 10:38:56.304692','5','intro',2,'Changed title_content.',7,1),(65,'2017-08-04 10:49:47.604771','17','description',2,'Changed title_content.',7,1),(66,'2017-08-04 11:18:08.050292','9','advantage',2,'Changed title_content.',7,1),(67,'2017-08-04 12:04:34.751522','24','privatedis',2,'Changed title_content.',7,1),(68,'2017-08-04 12:11:11.733332','23','publicdis',2,'Changed title_content.',7,1),(69,'2017-08-04 13:15:28.624839','5','intro',2,'Changed title_content.',7,1),(70,'2017-08-04 13:17:26.166700','17','description',2,'Changed title_content.',7,1),(71,'2017-08-04 13:38:34.300270','21','publicorprivate',2,'Changed title_content.',7,1);
/*!40000 ALTER TABLE `django_admin_log` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_content_type`
--

DROP TABLE IF EXISTS `django_content_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_content_type` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `app_label` varchar(100) NOT NULL,
  `model` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `django_content_type_app_label_76bd3d3b_uniq` (`app_label`,`model`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_content_type`
--

LOCK TABLES `django_content_type` WRITE;
/*!40000 ALTER TABLE `django_content_type` DISABLE KEYS */;
INSERT INTO `django_content_type` VALUES (1,'admin','logentry'),(3,'auth','group'),(2,'auth','permission'),(4,'auth','user'),(7,'CloudasaService','material'),(5,'contenttypes','contenttype'),(6,'sessions','session');
/*!40000 ALTER TABLE `django_content_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_migrations`
--

DROP TABLE IF EXISTS `django_migrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_migrations` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `app` varchar(255) NOT NULL,
  `name` varchar(255) NOT NULL,
  `applied` datetime(6) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=15 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_migrations`
--

LOCK TABLES `django_migrations` WRITE;
/*!40000 ALTER TABLE `django_migrations` DISABLE KEYS */;
INSERT INTO `django_migrations` VALUES (1,'contenttypes','0001_initial','2017-05-02 17:19:00.662785'),(2,'auth','0001_initial','2017-05-02 17:19:10.901705'),(3,'admin','0001_initial','2017-05-02 17:19:13.575311'),(4,'admin','0002_logentry_remove_auto_add','2017-05-02 17:19:13.701087'),(5,'contenttypes','0002_remove_content_type_name','2017-05-02 17:19:14.921636'),(6,'auth','0002_alter_permission_name_max_length','2017-05-02 17:19:16.232162'),(7,'auth','0003_alter_user_email_max_length','2017-05-02 17:19:17.410863'),(8,'auth','0004_alter_user_username_opts','2017-05-02 17:19:17.479224'),(9,'auth','0005_alter_user_last_login_null','2017-05-02 17:19:18.121534'),(10,'auth','0006_require_contenttypes_0002','2017-05-02 17:19:18.171291'),(11,'auth','0007_alter_validators_add_error_messages','2017-05-02 17:19:18.238951'),(12,'sessions','0001_initial','2017-05-02 17:19:18.981846'),(13,'CloudasaService','0001_initial','2017-05-02 17:21:28.944419'),(14,'CloudasaService','0002_auto_20170720_1750','2017-07-20 17:50:21.151715');
/*!40000 ALTER TABLE `django_migrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_session`
--

DROP TABLE IF EXISTS `django_session`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_session` (
  `session_key` varchar(40) NOT NULL,
  `session_data` longtext NOT NULL,
  `expire_date` datetime(6) NOT NULL,
  PRIMARY KEY (`session_key`),
  KEY `django_session_de54fa62` (`expire_date`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_session`
--

LOCK TABLES `django_session` WRITE;
/*!40000 ALTER TABLE `django_session` DISABLE KEYS */;
INSERT INTO `django_session` VALUES ('5zucbxnvv7e0o08zs2yhyel5t3drquoy','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-06-15 13:36:38.099698'),('7ndm04rcep6m2jake2nsnys65w4mo74e','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-06-27 18:34:46.688157'),('blf3d759zhrexqrfgrvh43lhfp3l11il','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-05-23 19:11:28.637749'),('lnhv7euh5oyp1tcl3n11oz72o1re1wtf','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-08-18 10:37:52.099441'),('qqpzprvmw5wkmqpopdk3afedb3oqqa6a','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-07-24 18:32:22.245093'),('r2u2vkx7a4r32twsogocoemwe2xhvhp1','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-05-17 17:09:22.206223'),('xc3amizu1vm013x1romyvksimhcxl8zi','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-07-29 18:43:45.204082'),('zxwxtvpq94sct01vbgl6hjtpl9d9h5v7','Yjg1NDMwMjAyYjc0ZTNhMzczOGNmMGRhOWNkZTBhNGZhN2MxYjVjNzp7Il9hdXRoX3VzZXJfaGFzaCI6ImRmZWQ1NzFhNzBjYzA5MzBjZGQ0NTlhYmIyMGUxZDY0NjgyM2Y4NmIiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-06-30 10:16:56.257049');
/*!40000 ALTER TABLE `django_session` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-09-16 16:50:08
